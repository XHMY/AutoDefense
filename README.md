# AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks

[**Blog**](https://microsoft.github.io/autogen/0.2/blog/2024/03/11/AutoDefense/Defending%20LLMs%20Against%20Jailbreak%20Attacks%20with%20AutoDefense/)

## Installation

```bash
pip install vllm autogen pandas retry openai
```

## Prepare Inference Service Using [vLLM](https://docs.vllm.ai/)

vLLM provides an OpenAI-compatible API server with efficient inference and built-in load balancing across multiple GPUs.

### Start vLLM Server

Start the vLLM server with your desired model. For multi-GPU setups, use `--data-parallel-size` to enable automatic load balancing:

**Single GPU:**
```bash
vllm serve meta-llama/Llama-2-13b-chat-hf --port 8000
```

**Multiple GPUs (e.g., 4 GPUs with data parallelism):**
```bash
vllm serve meta-llama/Llama-2-13b-chat-hf --port 8000 --data-parallel-size 4
```

**With tensor parallelism for larger models:**
```bash
vllm serve meta-llama/Llama-2-70b-chat-hf --port 8000 --tensor-parallel-size 4
```

**Combined tensor and data parallelism (8 GPUs, 2-way TP Ã— 4-way DP):**
```bash
vllm serve meta-llama/Llama-2-70b-chat-hf --port 8000 --tensor-parallel-size 2 --data-parallel-size 4
```

For more details on data parallel deployment with internal load balancing, see the [vLLM documentation](https://docs.vllm.ai/en/latest/serving/data_parallel_deployment/).

### Verify the Server

You can verify the server is running by checking the models endpoint:

```bash
curl http://localhost:8000/v1/models
```

## Response Generation

The responses are generated by GPT-3.5-Turbo. Please fill in the token information in `data/config/llm_config_list.json` before running the following command.

```bash
python attack/attack.py
```

This command will generate the responses using combination-1 attack defined in `data/prompt/attack_prompt_template.json`.
It attacks 5 times using different seeds by default.

## Run Defense Experiments

The following command runs the experiments of 1-Agent, 2-Agent, and 3-Agent defense. The `data/harmful_output/gpt-35-turbo-1106/attack-dan.json` represents all `json` files starting with `attack-dan` in the folder `data/harmful_output/gpt-35-turbo-1106/`. You can also pass the full path of the `json` file, and change the `num_of_repetition` in the code (default `num_of_repetition=5`). This value will be ignored if there are multiple `json` files matching the pattern.

```bash
python defense/run_defense_exp.py --model_list llama-2-13b \
--output_suffix _fp0-dan --temperature 0.7 --frequency_penalty 0.0 --presence_penalty 0.0 \
--eval_harm --chat_file data/harmful_output/gpt-35-turbo-1106/attack-dan.json \
--host_name 127.0.0.1 --port 8000
```

### Command Line Arguments

| Argument | Description | Default |
|----------|-------------|---------|
| `--model_list` | List of models to evaluate | `gpt-3.5-turbo-1106` |
| `--chat_file` | Path to the chat file with harmful outputs | `data/harmful_output/attack_gpt3.5_1106.json` |
| `--port` | Port where vLLM server is running | `8000` |
| `--host_name` | Hostname(s) of the vLLM server | `127.0.0.1` |
| `--output_suffix` | Suffix for output directory | `""` |
| `--frequency_penalty` | Frequency penalty for generation | `0.0` |
| `--presence_penalty` | Presence penalty for generation | `0.0` |
| `--temperature` | Temperature for generation | `0.7` |
| `--eval_harm` | Evaluate on harmful prompts | `False` |
| `--eval_safe` | Evaluate on safe prompts | `False` |

After finishing the defense experiment, the output will appear in the `data/defense_output/open-llm-defense_fp0-dan/llama-2-13b` folder.

## GPT-4 Evaluation

Evaluating harmful output defense:

```bash
python evaluator/gpt4_evaluator.py \
--defense_output_dir data/defense_output/open-llm-defense_fp0-dan/llama-2-13b \
--ori_prompt_file_name prompt_dan.json
```

After finishing the evaluation, the output will appear in the `data/defense_output/open-llm-defense_fp0-dan/llama-2-13b/asr.csv`.
There will be also a `score` value appearing for each defense output in the output `json` file.
Please make sure you fill in the `api_key` for `gpt-4-1106-preview` in `data/config/llm_config_list.json` before running the following command. The GPT-4 evaluation is very costly. We have enabled the cache mechanism to avoid repeated evaluation.

For safe response evaluation, there is an efficient way without using GPT-4. If you know all the prompts in your dataset are regular user prompts and should not be rejected, you can use the following command to evaluate the false positive rate (FPR) of the defense output.

```bash
python evaluator/evaluate_safe.py
```

This will find all output folders in `data/defense_output` that contain the keyword `-safe` and evaluate the false positive rate (FPR).
The FPR will be saved in the `data/defense_output/defense_fp.csv` file.
